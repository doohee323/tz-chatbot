# 슬라이드 02: 임베딩과 벡터 검색

## 슬라이드 내용 (한 장)

**임베딩 (Embedding)**  
• 텍스트를 **고정 차원의 벡터(숫자 배열)** 로 변환  
• “의미가 비슷한 텍스트 → 벡터도 가깝게” 되도록 학습된 모델 사용  
• 본 프로젝트: OpenAI 또는 Google Gemini (환경변수로 선택)

**벡터 검색**  
• 질의도 같은 임베딩 모델로 벡터화  
• 벡터 DB에서 **질의 벡터와 가장 가까운** 벡터(코사인 유사도 등) 검색 → 해당 **문서 청크** 반환  
• 그 청크를 LLM “참고 자료”로 전달해 답변 생성

**청킹 (Chunking)**  
• 긴 문서를 **일정 크기 단위(청크)** 로 분할  
• tz-chatbot: LangChain **RecursiveCharacterTextSplitter** (chunk_size, chunk_overlap 등 환경변수)

---

## 발표 노트

임베딩은 텍스트를 고정 차원의 벡터, 즉 숫자 배열로 바꾸는 과정입니다. 의미가 비슷한 텍스트는 벡터 공간에서도 가깝게 놓이도록 학습된 임베딩 모델을 씁니다. 본 프로젝트에서는 OpenAI나 Google Gemini 임베딩을 환경변수로 선택해서 사용합니다.

벡터 검색은 이렇게 동작합니다. 사용자 질의를 같은 임베딩 모델로 벡터화하고, 벡터 DB에서 질의 벡터와 가장 가까운 벡터들을 찾습니다. 보통 코사인 유사도 같은 걸 쓰고, 그에 해당하는 문서 청크를 가져옵니다. 그 청크들을 LLM에게 참고 자료로 넘겨서 답변을 생성하는 거죠. 그래서 임베딩 품질과 청크 크기가 RAG 품질에 영향을 많이 줍니다.

청킹은 긴 문서를 일정 크기 단위로 자르는 것입니다. 너무 크면 검색 정확도나 토큰 제한에 불리하고, 너무 작으면 문맥이 깨집니다. tz-chatbot에서는 LangChain의 RecursiveCharacterTextSplitter를 쓰고, chunk_size나 chunk_overlap은 환경변수로 조정합니다. 문단·문장 경계를 고려해서 자르기 때문에 단순 슬라이싱보다 품질이 좋습니다. 이 부분은 섹션 8에서 ingest 스크립트와 함께 다시 다룹니다.
